# -*- coding: utf-8 -*-
"""N-GRAM NAIVE BAYES

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TxQFHbpkKLBM9KOHzy1MSoscIOJZqF4D
"""

pip install scikit-learn pandas

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from textblob import TextBlob
from io import BytesIO
from google.colab import files

!gdown 1k8yaPTHgrSosu38Fe84KEg_HemCV82p3
!gdown 1mnq0GrWz-YvWWoYul_LNC3qLED2cxzfR

teksu = pd.read_csv('TEKS - Sheet1.csv')['Teks']
print(len(teksu))
labelsu = pd.read_csv('LABEL - Sheet1.csv')['Label']
print(len(labelsu))

data = pd.DataFrame({'text': teksu, 'label': labelsu})

print(data)

train_data, test_data, train_labels, test_labels = train_test_split(data['text'], data['label'], train_size=0.8, random_state=40)

model = make_pipeline(
    CountVectorizer(ngram_range=(1, 2)),
    TfidfTransformer(),
    MultinomialNB()
)

model.fit(train_data, train_labels)

predicted_labels = model.predict(test_data)

print("Accuracy:", accuracy_score(test_labels, predicted_labels))
print("\nClassification Report:\n", classification_report(test_labels, predicted_labels))
print("\nConfusion Matrix:\n", confusion_matrix(test_labels, predicted_labels))

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, data['text'], data['label'], cv=cv, scoring='accuracy')

print("\nCross-Validation Accuracy Scores:", cv_scores)
print("Mean Accuracy:", cv_scores.mean())

vectorizer = CountVectorizer(ngram_range=(1, 2))
X_train = vectorizer.fit_transform(train_data)
X_test = vectorizer.transform(test_data)

feature_names = vectorizer.get_feature_names_out()
print("\nFeatures (Top 10):\n", feature_names[:10])

import pandas as pd
import matplotlib.pyplot as plt
from nltk import ngrams
from collections import Counter

texts = data['text']

def extract_ngrams(text, n):
    n_grams = ngrams(text.split(), n)
    return [' '.join(gram) for gram in n_grams]


n_values = [1, 2]


fig, axes = plt.subplots(nrows=len(n_values), ncols=1, figsize=(10, 6))

for i, n in enumerate(n_values):

    all_ngrams = [extract_ngrams(text, n) for text in texts]


    flattened_ngrams = [gram for sublist in all_ngrams for gram in sublist]


    ngram_counts = Counter(flattened_ngrams)


    top_ngrams = ngram_counts.most_common(10)
    labels, counts = zip(*top_ngrams)

    axes[i].bar(labels, counts)
    axes[i].set_title(f'Top 10 {n}-grams')
    axes[i].set_xlabel('N-gram')
    axes[i].set_ylabel('Count')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(data['text'].str.cat())

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()